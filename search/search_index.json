{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Red Hat OpenShift","text":"<p>In this tutorial, you will walk through an introduction to OpenShift Container Platform (OCP) from both the web console and the OpenShift command line interface, <code>oc</code>. This tutorial is based on the Getting Started portion of the OpenShift Container Platform documentation, but modified to run on various platform architectures (including IBM Z, IBM Power, and x86) as well as adding more explanations for the concepts it covers.</p>"},{"location":"#pre-requisites","title":"Pre-Requisites","text":"<ol> <li>Access to an OpenShift cluster</li> <li><code>oc</code> CLI installed</li> <li>Projects created for each user with the names <code>userNN-project</code> (<code>NN</code> being each user number). Administrators can create this ahead of time with any limitations or quotas they wish, or administrators can allow users to create their own projects.</li> </ol> <p>If you are going through this lab during a workshop provided by the IBM Z Washington Systems Center, these pre-requisites are already fulfilled in the environment you were provided.</p>"},{"location":"access/","title":"Accessing the Environment","text":""},{"location":"access/#accessing-your-virtual-machine","title":"Accessing your Virtual Machine","text":"<ol> <li>Access the workshop Attendee URL here</li> <li>Log in with your IBM ID.</li> <li>If you don't already have an IBM ID, you will need to create one here</li> <li>When prompted, enter the workshop password: <code>password</code></li> <li>Open your virtual machine using the big blue button under 'VM Remote Console'.</li> </ol> <ol> <li>Log into the RHEL virtual machine with password: <code>p@ssw0rd</code> (that's a zero). You may already be logged in when first accessing the VM.</li> </ol> <p>You may need to hit your <code>Enter</code> key to reach the login page.</p> <p>If you run into an issue where your mouse pointer is not visible, try to open the Virtual Machine in a different browser. If that doesn't work, try to open the VM in a new window with the button in the top right of the VM.</p> <p>Please do not log off or reboot the Virtual Machine, as that will disconnect the VPN.</p>"},{"location":"access/#accessing-openshift-web-console","title":"Accessing OpenShift web console","text":"<ol> <li>Go to the OpenShift Cluster here</li> <li>Use your credentials in the table below. </li> </ol>"},{"location":"access/#connecting-to-openshift-via-the-cli","title":"Connecting to OpenShift via the CLI","text":"<p>To access OpenShift via the CLI, you must first <code>ssh</code> to one of our WSC Linux guests from the Windows Virtual Machine.</p> <pre><code>ssh userNN@192.168.176.61\n</code></pre> <p>Then paste your <code>oc login</code> command.</p>"},{"location":"access/#openshift-credentials","title":"OpenShift credentials","text":"<p>You can find your Environment Number on the TechZone Workshop page in the 'Your Environment' section.</p> <p></p> Environment Number NN Username Password 1 01 <code>user01</code> <code>p@ssw0rd</code> 2 02 <code>user02</code> <code>p@ssw0rd</code> 3 03 <code>user03</code> <code>p@ssw0rd</code> 4 04 <code>user04</code> <code>p@ssw0rd</code> 5 05 <code>user05</code> <code>p@ssw0rd</code> 6 06 <code>user06</code> <code>p@ssw0rd</code> 7 07 <code>user07</code> <code>p@ssw0rd</code> 8 08 <code>user08</code> <code>p@ssw0rd</code> 9 09 <code>user09</code> <code>p@ssw0rd</code> 10 10 <code>user10</code> <code>p@ssw0rd</code> 11 11 <code>user11</code> <code>p@ssw0rd</code> 12 12 <code>user12</code> <code>p@ssw0rd</code> 13 13 <code>user13</code> <code>p@ssw0rd</code> 14 14 <code>user14</code> <code>p@ssw0rd</code> 15 15 <code>user15</code> <code>p@ssw0rd</code> 16 16 <code>user16</code> <code>p@ssw0rd</code> 17 17 <code>user17</code> <code>p@ssw0rd</code> 18 18 <code>user18</code> <code>p@ssw0rd</code> 19 19 <code>user19</code> <code>p@ssw0rd</code> 20 20 <code>user20</code> <code>p@ssw0rd</code> 21 21 <code>user21</code> <code>p@ssw0rd</code> 22 22 <code>user22</code> <code>p@ssw0rd</code> 23 23 <code>user23</code> <code>p@ssw0rd</code> 24 24 <code>user24</code> <code>p@ssw0rd</code> 25 25 <code>user25</code> <code>p@ssw0rd</code> 26 26 <code>user26</code> <code>p@ssw0rd</code> 27 27 <code>user27</code> <code>p@ssw0rd</code> 28 28 <code>user28</code> <code>p@ssw0rd</code> 29 29 <code>user29</code> <code>p@ssw0rd</code> 30 30 <code>user30</code> <code>p@ssw0rd</code>"},{"location":"administrator/","title":"The Administrator Perspective","text":"<p>Take a moment to notice the following elements in the navigation bar:</p> <p></p> <p>These buttons display on each page of the OpenShift console. Note that the Applications button might be missing from your screen, depending on your credentials and which applications are currently deployed on the cluster.</p> <p>By default, the menu on the left side of the page should be activated and displaying the cluster menu. However, if your screen is sized too small, you may need to click the Menu button to expand it.</p> <ol> <li> <p>In the left-side menu, select the Administrator perspective if it isn't already showing.</p> <p></p> <p>With the administrator menu showing, you are provided with a broad range of options to manage the OpenShift cluster and the applications running on it.</p> <p></p> <p>The Administrator perspective is the default view for the OpenShift console for users who have an administrative access level. This perspective provides visibility into options related to cluster administration, the cluster operators running the OpenShift cluster itself, as well as a broader view of the projects associated with the currently authenticated user.</p> <p>Your user credentials have the <code>cluster-reader</code> roleBinding. This is a read-only roleBinding that allows you to see most of what OpenShift has to offer administrators without allowing you to modify cluster objects. For example, typical non-admin users would not be able to complete the following few steps.</p> </li> <li> <p>In the menu at the very bottom, click Administration -&gt; Cluster Settings.</p> <p></p> <p>The cluster settings page is where administrators can see what OpenShift versions are available, and also update the cluster from within the console. OpenShift completely automates the cluster update once triggered by an administrator, including updating all of the cluster operators and the CoreOS operating system running on the nodes.</p> </li> <li> <p>On the Cluster Settings page, select the ClusterOperators tab.</p> <p></p> <p>Each clusterOperator is responsible for managing resources related to a specific OpenShift function. For example, the <code>authentication</code> clusterOperator manages all of the resources (pods, routes, secrets, etc.) related to the LDAP authentication you are using to log in with the <code>userNN</code> username.</p> </li> <li> <p>In the Menu, click Compute -&gt; Nodes.</p> <p></p> <p>As you see on the Nodes page, the OpenShift cluster is made up of control plane nodes and compute nodes.</p> <ul> <li>Control Plane nodes are responsible for most of the core Kubernetes and OpenShift services such as etcd, the Kubernetes and OpenShift apiservers, the scheduler, and more. There are all vital tasks that make it possible to run the cluster and schedule application workloads to Compute Nodes.</li> <li>Compute nodes are where application containers run. The containers for the sample application that you will build and deploy later in this lab will run on the compute nodes.</li> </ul> <p>In production environments, nodes are typically run on multiple machines (physical and/or virtual) in order to be more highly available and fault-tolerant.</p> </li> <li> <p>On the Nodes page, click the hyperlink for one of the Compute nodes.</p> <p></p> <p>When looking at a specific node, you are provided a view similar to the OpenShift cluster overview page, but now it is scoped to display only the pods, events, metrics, etc. for the specific node of interest.</p> </li> <li> <p>On the Compute Node's page, click the Details tab.</p> <p></p> <p>On the node details tab, you are provided more information about the node including the architecture it is running on. In the screenshot above, the <code>s390x</code> architecture indicates that the node is running the Linux on IBM Z (<code>s390x</code>) architecture.</p> <p>One of the many benefits of OpenShift (and Kubernetes more broadly) is that they abstract away the underlying infrastructure from the end users - developers and Kubernetes administrators. It does not take any traditional or specialized skills to run containers on platforms such as IBM Z.</p> </li> <li> <p>In the Menu, click Home -&gt; Projects.</p> <p></p> <p>The rest of the page is populated by projects. A project has been created for you to work in named <code>userNN-project</code> (where <code>NN</code> is your user number). If your OpenShift administrator did not create this project for you, you should be able to do so yourself with the <code>Create Project</code> button in the top right of the projects page.</p> <p>Any project starting with <code>openshift-</code> or <code>kube-</code> contain the workloads running the OpenShift platform itself.</p> </li> <li> <p>Click the userNN-project hyperlink (where <code>NN</code> is your user number).</p> <p>Tip: With so many Projects displayed, you can use the search bar to find yours more easily.</p> <p>You will now see the Dashboard for your project.</p> <p></p> <p>OpenShift automatically creates a few special serviceAccounts in every project. The <code>default</code> serviceAccount takes responsibility for running the pods. OpenShift uses and injects this serviceAccount into every pod that launches.</p> <p>The following step creates a RoleBinding object for the <code>default</code> serviceAccount object. The serviceAccount communicates with the OpenShift Container Platform API to learn about pods, services, and resources within the project.</p> </li> <li> <p>Click the RoleBindings tab of your project, and then click Create Binding.</p> <p></p> </li> <li> <p>Fill out the form as follows.</p> <ul> <li>Name: <code>sa-user-account</code></li> <li>Role Name: <code>view</code></li> <li>Subject: <code>ServiceAccount</code></li> <li>Subject namespace: <code>userNN-project</code> where <code>NN</code> is your user number</li> <li>Subject Name: <code>default</code></li> </ul> <p></p> <p>Then click create.</p> <p>Now, the pods you create in this project that use the <code>default</code> serviceAccount will have the proper permissions needed for the sample application.</p> </li> <li> <p>Look through the Overview tab of your project.</p> <p>This displays information about what\u2019s going on in your project, such as CPU and memory usage, any alerts or crashlooping pods, an inventory of all the Kubernetes resources deployed in the project, and more. You won\u2019t see much information yet, as no workloads should be running in this project.</p> </li> <li> <p>Click the Workloads tab to the right of YAML.</p> <p>This page displays all of the workloads in your project, so it\u2019s empty for now.</p> <p>All objects in OpenShift can be generated using YAML files. YAML is a human-readable language for configuration files. Most OpenShift object such as Deployments, Services, Routes, etc. can be modified by directly editing their YAML file in either the console or command line.</p> <p>Workloads are typically created by developers, so in the next section, you will swap to the developer perspective to deploy an application. You will return to the administrator perspective later in this lab.</p> </li> </ol>"},{"location":"cli/","title":"Introduction to the <code>oc</code> CLI","text":"<p>The <code>oc</code> CLI is an incredibly powerful tool to manage OpenShift clusters and applications. It is capable of performing any task you can do in the web console, and some tasks are only possible to do with <code>oc</code>.</p> <p><code>oc</code> is ideal in situations where you:</p> <ul> <li>Work directly with project source code.</li> <li>Script OpenShift Container Platform operations.</li> <li>Are restricted by bandwidth resources and cannot use the web console.</li> </ul> <p>Furthermore, many people familiar with Linux and/or Kubernetes tend to find the <code>oc</code> command line an easier and more efficient method of performing tasks, rather than the web-based console.</p> <p>Like with the OpenShift web console, the OpenShift command line includes functions both for developers and for administrators.</p> <p>For those who are familiar with Kubernetes and its <code>kubectl</code> CLI tool, the <code>oc</code> tool is essentially analogous but with additional features.</p>"},{"location":"cli/#logging-in-with-the-oc-cli","title":"Logging in with the <code>oc</code> CLI","text":"<p>The frontend application, <code>parksmap</code>, needs a backend. In this section, you will deploy a python application named <code>nationalparks</code>. This application performs 2D geo-spatial queries against a MongoDB database to locate and return map coordinates of all national parks in North America.</p> <p>NOTE for those in the WSC hands-on lab session, you must complete the following steps from within the WSC linux guest. Refer to the access.md page. If you try to connect directly from the RHEL virtual machine terminal, the command will result in an error.</p> <ol> <li> <p>From the OpenShift web console, click your username in the top right corner (i.e. <code>Workshop User NN</code>) and select <code>Copy login command</code>.</p> <p></p> </li> <li> <p>Log in with your OpenShift credentials and click the Display Token hyperlink.</p> </li> </ol> <p>NOTE if the copied command fails, try adding the following flag to the end of the <code>oc login command</code>:</p> <pre><code>--insecure-skip-tls-verify\n</code></pre> <ol> <li> <p>Copy the line that begins with <code>oc login</code>, paste it into a terminal session, and execute the command.</p> <p>Sample output: </p> <pre><code>$ oc login --token=uL3fHEPSGH3io0kljsgRfAMAPIIY44BhwnGxCMA3dei4 --server=https://api.example.com:6443\nLogged into \"https://api.example.com:6443\" as \"user01\" using the token provided.\n\nYou have access to 161 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"user01-project\".\n</code></pre> <p>You are now logged into the cluster via the <code>oc</code> CLI, and you are told which project you are using.</p> <p>If you\u2019re in a project other than your own <code>userNN-project</code>, use the following command to move into it: <code>oc project userNN-project</code>, where <code>NN</code> is your user number.</p> </li> </ol>"},{"location":"cli/#exploring-the-oc-cli","title":"Exploring the <code>oc</code> CLI","text":"<ol> <li> <p>In your terminal, test out the <code>oc</code> CLI.</p> <pre><code>oc --help\n</code></pre> <p>Sample output: </p> <pre><code>user01@lab061:~$ oc --help\nOpenShift Client\n\nThis client helps you develop, build, deploy, and run your applications on any\nOpenShift or Kubernetes cluster. It also includes the administrative\ncommands for managing a cluster under the 'adm' subcommand.\n\nUsage:\noc [flags]\n\nBasic Commands:\nlogin           Log in to a server\nnew-project     Request a new project\nnew-app         Create a new application\nstatus          Show an overview of the current project\nproject         Switch to another project\nprojects        Display existing projects\nexplain         Documentation of resources\n\nBuild and Deploy Commands:\nrollout         Manage a Kubernetes deployment or OpenShift deployment config\nrollback        Revert part of an application back to a previous deployment\nnew-build       Create a new build configuration\nstart-build     Start a new build\n</code></pre> <p>The <code>--help</code> flag will display all of the available options the oc CLI.</p> </li> <li> <p>Enter the following command</p> <pre><code>oc new-app --help\n</code></pre> <p>Sample output:</p> <pre><code>user01@lab061:~$ oc new-app --help\nCreate a new application by specifying source code, templates, and/or images\n\nThis command will try to build up the components of an application using images, templates, or code\nthat has a public repository. It will lookup the images on the local Docker installation (if\navailable), a container image registry, an integrated image stream, or stored templates.\n\nIf you specify a source code URL, it will set up a build that takes your source code and converts\nit into an image that can run inside of a pod. Local source must be in a git repository that has a\nremote repository that the server can see. The images will be deployed via a deployment\nconfiguration, and a service will be connected to the first public port of the app. You may either\nspecify components using the various existing flags or let new-app autodetect what kind of\ncomponents you have provided.\n\nIf you provide source code, a new build will be automatically triggered. You can use 'oc status' to\ncheck the progress.\n\nUsage:\noc new-app (IMAGE | IMAGESTREAM | TEMPLATE | PATH | URL ...) [flags]\n\nExamples:\n# List all local templates and image streams that can be used to create an app\noc new-app --list\n\n# Create an application based on the source code in the current git repository (with a public\nremote) and a Docker image\noc new-app . --docker-image=repo/langimage\n</code></pre> <p>The <code>--help</code> flag now displays all of the available options for the oc new-app command. If you get confused about any of the commands we use in this workshop, or just want more information, using this flag is a good first step.</p> </li> </ol> <p><code>oc new-app</code> is a powerful and commonly used command in the OpenShift CLI. It has the ability to deploy applications from components that include:</p> <ul> <li>Source or binary code</li> <li>Container images</li> <li>Templates</li> </ul> <p>The set of objects created by <code>oc new-app</code> depends on the artifacts passed as an input.</p>"},{"location":"cli/#use-oc-to-deploy-a-python-application-from-source-code","title":"Use <code>oc</code> to Deploy a Python Application from Source Code","text":"<p>The frontend application, <code>parksmap</code>, needs a backend. In this section, you will deploy a python application named <code>nationalparks</code>. This application performs 2D geo-spatial queries against a MongoDB database to locate and return map coordinates of all national parks in North America.</p> <ol> <li> <p>Deploy the python backend with the following <code>oc new-app</code> command.</p> <pre><code>oc new-app python~https://github.com/mmondics/national-parks --context-dir source/nationalparks-py --name nationalparks -l 'app=national-parks-app,component=nationalparks,role=backend,app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=python'\n</code></pre> <p>A few things to notice about this command:</p> <ul> <li>The <code>oc new-app</code> command is not being run against a specific container image. It is being run against python source code that exists in GitHub here. Although a Dockerfile exists in the directory, it is not being used due to the <code>pyhon~</code> option in the command. OpenShift is using its source-to-image capability to create its own Dockerfile and containerize the application from its source code. If you wanted to, you could have omitted the <code>python~</code> option and OpenShift would use the Dockerfile in the directory.</li> <li><code>--name</code> flag provides the name for the python Deployment</li> <li><code>-l</code> sets the following key=value pairs as labels on the deployment</li> </ul> <p>The output from the <code>oc new-app</code> command tells you what all was created - an imagestream, buildconfig, deployment, and service. </p> <p>The buildconfig is the configuration file that will be used to build the <code>nationalparks</code> container image. This build will automatically begin, and you can check its logs to watch the process. </p> </li> <li> <p>Check the <code>nationalparks</code> build log.</p> <pre><code>oc logs build/nationalparks-1 -f\n</code></pre> <p>The <code>-f</code> flag lets you follow the build logs (similar to Linux's 'tail' command). Once you see <code>Push successful</code> at the end of the build logs, your new container image has been built and pushed into OpenShift's internal registry. It will then automatically be deployed in a pod.</p> </li> <li> <p>Check that the <code>nationalparks</code> pod is running and ready.</p> <pre><code>oc get pods\n</code></pre> <p>Sample output:</p> <pre><code>\u279c  ~ oc get pods\nNAME                             READY   STATUS      RESTARTS   AGE\nnationalparks-1-build            0/1     Completed   0          5m19s\nnationalparks-64d57bb8f8-4582b   1/1     Running     0          4m6s\nparksmap-cbc66fb69-d29rn         1/1     Running     0          3m25s\nparksmap-cbc66fb69-dmnf7         1/1     Running     0          3m24s\n</code></pre> <p>Once the <code>nationalparks</code> pod is <code>Running</code> and has <code>1/1</code> containers ready, the application is successfully deployed. However, the backend python application is only accessible from within the OCP cluster. It is not exposed to the outside world, as you may have noticed from the output of the <code>oc new-app</code> command: <code>Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:</code></p> </li> <li> <p>Create a route that exposes the <code>nationalparks</code> service.</p> <pre><code>oc expose service/nationalparks\n</code></pre> <p>It's worth understanding a little bit about what you just did. The backend application <code>nationalparks</code> runs in one pod. However, you could easily scale the number of pods running <code>nationalparks</code> up as much as you'd like, or you can let OpenShift automatically scale out the number of pods (based on CPU or memory consumption) with a HorizontalPodAutoscaler. Each pod will be assigned its own individual IP address, and furthermore, if and when the pods regenerate, they will get brand new IP addresses. For these reasons, you cannot rely on the application's IP address in order to use it, like you would with many non-containerized applications. </p> <p>To solve this problem, Kubernetes uses services. Services are load balancers internal to the cluster that distribute requests among the application pods using label matching. If you want to access an application, you need to access the service that will then direct the request to one of the pods.</p> <p>However, services are internal to the cluster. They allow pods to communicate with other pods inside the cluster, but not with the outside world. For external access, we need to introduce another object - routes. </p> <p>Routes are OpenShift objects - they do not exist in upstream Kubernetes. Routes expose services as publicly-accessible addresses for users and applications to interact with. When you access an OpenShift appliction in a web browser, such as the <code>parksmap</code> webpage or even the OpenShift console, you navigate to that pod's route.</p> <p></p> </li> <li> <p>See the new route that was created.</p> <pre><code>oc get route\n</code></pre> <p>Sample output:</p> <pre><code>\u279c  ~ oc get routes\nNAME            HOST/PORT                                        PATH   SERVICES        PORT       TERMINATION     WILDCARD\nnationalparks   nationalparks-user01-project.apps.example.com          nationalparks   8080-tcp                   None\nparksmap        parksmap-user01-project.apps.example.com               parksmap        8080-tcp   edge/Redirect   None\n</code></pre> </li> <li> <p>Label the <code>nationalparks</code> route as the application backend.</p> <pre><code>oc label route nationalparks type=parksmap-backend\n</code></pre> </li> <li> <p>Navigate to the frontend <code>parksmap</code> route in a web browser. Use the <code>parksmap</code> <code>HOST/PORT</code> value from the <code>oc get routes</code> command preceded by <code>http://</code>.</p> <p>For example: <code>http://parksmap-userNN-project.apps.example.com</code></p> <p>Alternatively, you can click the small arrow button associated with the <code>parksmap</code> deployment in the OpenShift web console topology.</p> <p></p> <p>Any way that you get there, your application should display the image below:</p> <p></p> <p>You should see a map of the world, but you will not see any icons for National Parks yet. That's because we have one more application to deploy and configure - the MongoDB database that will store all of the data about National Parks and their locations.</p> </li> </ol>"},{"location":"cli/#deploying-and-configuring-mongodb","title":"Deploying and Configuring MongoDB","text":"<p>The MongoDB you will deploy in this section will store all information about the National Parks, their names and coordinates.</p> <ol> <li> <p>Deploy the MongoDB container.</p> <pre><code>oc new-app quay.io/mmondics/mongodb:latest --name mongodb-nationalparks -e MONGODB_USER=mongodb -e MONGODB_PASSWORD=mongodb -e MONGODB_DATABASE=mongodb -e MONGODB_ADMIN_PASSWORD=mongodb -l 'app.kubernetes.io/part-of=national-parks-app,app.kubernetes.io/name=mongodb'\n</code></pre> <p>A few things to notice about this command:</p> <ul> <li>You're deploying a pre-built container image hosted at .  <li><code>--name</code> sets the name of the MongoDB deployment</li> <li><code>-e</code> sets environment variables within the resulting container. These can be used like any other environment variable when the container is running, and you can see them by connecting to the pod and running the <code>env</code> command</li> <li><code>-l</code> sets labels for this deployment</li> <li> <p>In a later step, you will be adding persistent storage to this MongoDB deployment. Mounting storage requires a certain securityContext, so in preparation, please run the following command to add the proper securityContext.</p> <pre><code>oc patch deployment mongodb-nationalparks --type='json' -p='[{\"op\":\"add\",\"path\":\"/spec/template/spec/securityContext\",\"value\":{\"fsGroup\":184,\"runAsGroup\":184,\"runAsUser\":184}}]'\n</code></pre> <p>Once again, you can check <code>oc get pods</code> to see when the MongoDB pod is ready.</p> <pre><code>NAME                                     READY   STATUS      RESTARTS   AGE\nmongodb-nationalparks-5d8b6f99f7-4wwst   1/1     Running     0          8s\nnationalparks-1-build                    0/1     Completed   0          58m\nnationalparks-67b69fc9b7-z5znq           1/1     Running     0          56m\nparksmap-cbc66fb69-c87k7                 1/1     Running     0          39m\n</code></pre> <p>Now, you may think that you are ready to look at your frontend application again to see the backend data. However, this MongoDB requires authentication before it can be used, so in the next section, you will provide sensitive information to the application using OpenShift secrets.</p> </li>"},{"location":"cli/#providing-sensitive-application-with-secrets","title":"Providing Sensitive Application with Secrets","text":"<p>The Secret object provides a mechanism to hold sensitive information such as passwords, OpenShift Container Platform client configuration files, private source repository credentials, and so on. Secrets decouple sensitive content from the pods. You can mount secrets into containers using a volume plugin or the system can use secrets to perform actions on behalf of a pod. By default, secrets are stored unencrypted in etcd. Therefore secrets are securable by default, not secured. To safely secure secrets, you must turn on etcd encryption as well as configure proper role-based access controls to the secret objects and etcd. Read more about secrets here.</p> <p>The following procedure adds the secret <code>nationalparks-mongodb-parameters</code> and mounts it to the <code>nationalparks</code> workload.</p> <ol> <li> <p>Create a secret holding sensitive information (usernames and passwords).</p> <pre><code>oc create secret generic nationalparks-mongodb-parameters --from-literal=DATABASE_SERVICE_NAME=mongodb-nationalparks --from-literal=MONGODB_USER=mongodb --from-literal=MONGODB_PASSWORD=mongodb --from-literal=MONGODB_DATABASE=mongodb --from-literal=MONGODB_ADMIN_PASSWORD=mongodb\n</code></pre> </li> <li> <p>Update the environment variable to attach the <code>nationalparks-mongodb-parameters</code> secret to the <code>nationalparks</code> workload.</p> <pre><code>oc set env --from=secret/nationalparks-mongodb-parameters deploy/nationalparks\n</code></pre> <p>The <code>nationalparks</code> deployment will notice that a change has been made, and it will create a new pod with these changes applied. Check with <code>oc get pods</code> until the new pod is up and running.</p> </li> <li> <p>Once the <code>mongodb-nationalparks</code> pod is running and ready, run the following command to load National Park data into MongoDB.</p> <p>Make sure that you are in your own project before running the command.</p> <pre><code>oc exec $(oc get pods -l component=nationalparks | tail -n 1 | awk '{print $1;}') -- curl -s http://localhost:8080/ws/data/load\n</code></pre> <p>If you see <code>\"Items inserted in database: 204\"</code>, the data was successfully loaded.</p> </li> <li> <p>Finally, return to your frontend <code>parksmap</code> application in a web browser.</p> <p></p> </li> </ol> <p>This interactive map is the culmination of all the Kubernetes and OpenShift objects that you created throughout the course of this tutorial. You can click on each icon on the map to see more details about each National Park.</p>"},{"location":"developer/","title":"The Developer Perspective","text":"<ol> <li> <p>In the left-side Menu, click the Administrator dropdown, and select Developer.</p> <p></p> <p>The Developer perspective provides views and workflows specific to developer use cases, while hiding many of the cluster management options typically used by administrators. This perspective provides developers with a streamlined view of the options they typically use.</p> <p></p> </li> </ol> <p>Switching to the Developer perspective should take you to the Topology view. If this isn't the case, select the Topology item in the left-side menu. If no workloads are deployed in the selected project, options to start building an application or visit the +Add page or are displayed.</p> <ol> <li> <p>Click the +Add button in the menu.</p> <p></p> <p>There are multiple methods of deploying workloads from the OpenShift web browser, including from raw source code hosted in a Git repository, from a pre-built container image, or from an operator.</p> </li> </ol>"},{"location":"developer/#deploying-a-container-image","title":"Deploying a Container Image","text":"<p>The simplest way to deploy an application in OpenShift Container Platform is to run an existing container image. The following procedure deploys a front end component of an application called <code>national-parks-app</code>. This frontend web application displays an interactive map which shows the location of national parks across North America.</p> <ol> <li> <p>From the +Add view in the Developer perspective, click Container images.</p> </li> <li> <p>Enter the following values:</p> <ul> <li>Image name from external registry: <code>quay.io/mmondics/national-parks-frontend:latest</code></li> <li>Application name: <code>national-parks-app</code></li> <li>Name: <code>parksmap</code></li> </ul> <p>Keep the default target port as well as the route option checked.</p> <p>In the Advanced Options section, click Labels and add labels to better identify this deployment later. Labels help identify and filter components in the web console and in the command line. Add the following labels:</p> <ul> <li><code>app=national-parks-app</code></li> <li><code>component=parksmap</code></li> <li><code>role=frontend</code></li> </ul> <p>Note: you can hit the <code>enter</code> or <code>tab</code> key to enter each individual label.</p> </li> <li> <p>Click Create.</p> <p>You are redirected to the Topology page where you will shortly see the <code>parksmap</code> deployment in the <code>national-parks-app</code> application.</p> <p></p> </li> </ol>"},{"location":"developer/#examining-the-pod","title":"Examining the Pod","text":"<p>The topology page gives you a visual representation of your application where you can view and monitor its components.</p> <ol> <li> <p>Click the circular icon for your <code>parksmap</code> deployment.</p> <p>This brings up a window on the right side of the screen with more options for the deployment.</p> </li> <li> <p>Click the details tab, if not already on it. </p> <p></p> </li> </ol> <p>Here you can manage its properties including number of copies, labels, or storage.</p> <ol> <li> <p>Click the Resources tab. </p> <p></p> <p>Here you can access the pod or its logs as well as the route where the application is accessible.</p> </li> <li> <p>Click the Actions dropdown menu.</p> <p></p> <p>This menu provides many options for how to manage or modify your <code>parksmap</code> deployment. Most of these options can be set either at the time you deploy the application, or afterwards as in this case.</p> </li> </ol>"},{"location":"developer/#scaling-the-frontend-application","title":"Scaling the Frontend Application","text":"<p>In Kubernetes, a Deployment object defines how an application deploys. In most cases, users use Pod, Service, ReplicaSets, and Deployment resources together. As in the previous section, OpenShift Container Platform creates each of these dependent resources for you, rather than you having to configure each one manually.</p> <p>When you deployed the <code>national-parks-frontend</code> image, a deployment resource was created with only one pod deployed. However, in most cases, users will want to scale their application to have multiple copies running at the same time. This is one of the core features of Kubernetes and OpenShift that build a more highly available application by creating multiple copies of it across different physical or virtual hosts.</p> <ol> <li> <p>On the details tab for the <code>parksmap</code> deployment, click the up arrow next to the blue circle that says <code>1 pod</code>. </p> <p></p> <p>This scales your application from one pod to two pods.</p> <p></p> <p>This is a simple demonstration of horizontal scaling with Kubernetes. You now have two instances of your pod running in the OpenShift cluster. Traffic to the <code>parksmap</code> application will now be distributed to each pod, and if for some reason a pod is lost, that traffic will be redistributed to the remaining pods until Kubernetes starts another. If a whole compute node is lost, Kubernetes will move the pods to different compute nodes.</p> <p>OpenShift and Kubernetes also support horizontal autoscaling of pods based on CPU or memory consumption, but that is outside the scope of this lab.</p> </li> </ol> <p>Now that you are more familiar with the OpenShift web console, the next section will introduce you to the OpenShift command line interface (CLI) <code>oc</code>. You will use the <code>oc</code> CLI to deploy a backend python application to serve data to the frontend <code>parksmap</code> as well as to deploy a containerized MongoDB application.</p>"},{"location":"finish/","title":"Finishing touches","text":""},{"location":"finish/#cleanup","title":"Cleanup","text":"<p>Once you're ready to clean up your OpenShift project, follow the instructions in this section.</p> <ol> <li> <p>Run the following command to clean up most of the objects in your project. Remember to change the value of <code>NN</code>.</p> <pre><code>oc delete all --all -n userNN-project\n</code></pre> <p>This will delete some of the objects in your project, but it will not delete the secret you created.</p> </li> <li> <p>Delete your secret with the following command.</p> <pre><code>oc delete secret nationalparks-mongodb-parameters -n userNN-project\n</code></pre> </li> </ol>"},{"location":"finish/#conclusion","title":"Conclusion","text":"<p>To learn more about OpenShift Container Platform and see the additional tooling it provides to develop and manage more complex applications, please refer to the following links.</p> <ul> <li>OpenShift 4.16 Documentation</li> <li>OpenShift Product Page</li> <li>Red Hat Hybrid Cloud Blog</li> <li>OpenShift Learning</li> <li>OpenShift on IBM Z Datasheet</li> </ul>"},{"location":"glossary/","title":"Glossary of Terms","text":"Term Definition Deployment A Kubernetes resource object that maintains the life cycle of an application. Containers Lightweight and executable images that consist software and all its dependencies. Because containers virtualize the operating system, you can run containers anywhere, from a data center to a public or private cloud to your local host. Node A worker machine in the OpenShift Container Platform cluster. A node is either a virtual machine (VM) or a physical machine. Operator The preferred method of packaging, deploying, and managing a Kubernetes application in an OpenShift Container Platform cluster. An Operator takes human operational knowledge and encodes it into software that is packaged and shared with customers. OperatorHub A platform that contains various OpenShift Container Platform Operators to install. Pod One or more containers with shared resources, such as volume and IP addresses, running in your OpenShift Container Platform cluster. A pod is the smallest compute unit defined, deployed, and managed. Role-Based Access Control (RBAC) A key security control to ensure that cluster users and workloads have only access to resources required to execute their roles. Route Routes expose a service to allow for network access to pods from users and applications outside the OpenShift Container Platform instance. Service A service exposes a running application on a set of pods. serviceAccount Object that provides an identity for processes running in a Pod. serviceAccounts authenticate against the Kubernetes API using their own credentials that are managed with Role-Based Access Control (RBAC)."},{"location":"monitoring/","title":"Metering, Monitoring, and Alerts","text":"<p>A significant architectural shift toward containers is underway and, as with any architectural shift, this brings new operational challenges. It can be challenging for many of the legacy monitoring tools to monitor container platforms in fast moving, often ephemeral environments. The good news is newer cloud-based offerings can ensure monitoring solutions are as scalable as the services being built and monitored. These new solutions have evolved to address the growing need to monitor your stack from the bottom to the top.</p> <p>From an operations point of view, infrastructure monitoring tools collect metrics about the host or container, such as CPU load, available memory and network I/O.</p> <p>The default monitoring stack is the 3-pronged open source approach of, Grafana, Alertmanager, and Prometheus.</p> <p>Prometheus gives you finely grained metrics at a huge scale. With the right configuration, Prometheus can handle millions of time series.</p> <p>Grafana can visualize the data being scraped by Prometheus. Grafana comes with pre-built dashboards for typical use cases, or you can create your own custom ones.</p> <p>Alertmanager forwards alerts to a service such as Slack or another webhook . Alertmanager can use metadata to classify alerts into groups such as errors, notifications, etc.</p> <p>The Grafana-Alertmanager-Prometheus monitoring stack provides a highly configurable, open source option to monitor Kubernetes workloads.</p> <p></p>"},{"location":"monitoring/#openshift-metrics-prometheus","title":"OpenShift Metrics (Prometheus)","text":"<p>OpenShift provides a web interface to Prometheus, which enables you to run Prometheus Query Language (PromQL) queries and visualize the metrics on a plot. This functionality provides an extensive overview of the cluster state and helps to troubleshoot problems.</p> <ol> <li> <p>In the OpenShift console, switch to the Administrator perspective if you are not already on it.</p> <p></p> </li> <li> <p>In the menu bar on the left side of the page, click Observe and then Metrics.</p> <p></p> <p>You will be taken to a Prometheus interface within the OpenShift console.</p> <p></p> <p>Once you enter a query, the graph will populate.</p> </li> <li> <p>Enter the following string in the query bar:</p> <pre><code>namespace:container_memory_usage_bytes:sum\n</code></pre> </li> <li> <p>Hit your enter key or click the associated query result that is returned.</p> <p>The string will populate the query text box.</p> </li> <li> <p>If the graph and table are not automatically populated. click the blue \"Run Queries\" button.</p> <p>The graph should now display the memory usage over time for each namespace.</p> <p></p> </li> <li> <p>Scroll down the page to the table displaying each namespace and its memory usage in bytes.</p> <p></p> <p>Your table will look different depending on what work is being done in the OpenShift cluster at the time.</p> <p>Notice that you have observability of the entire OpenShift cluster, even though you cannot access or edit projects other than your own. In other words, you have read-only access to the full OpenShift cluster via the Observability stack, but you read-write access within your userNN-project.</p> <p>OpenShift passes around a massive amount of data to run itself and the applications running on top of it. Prometheus is an extremely powerful data source that can return results for millions of time strings with extremely granular precision.</p> <p>Because of OpenShift\u2019s vast data production and Prometheus\u2019 ability to process it, certain queries can produce simply too much data to be useful. Because Prometheus makes use of labels, we can use these labels to filter data to make better sense of it.</p> </li> <li> <p>Modify your query to the following:</p> <pre><code>namespace:container_memory_usage_bytes:sum{namespace=\"userNN-project\"}\n</code></pre> <p>Make sure you change the one instance of <code>NN</code> to your user number.</p> <p>Also, notice that they are squiggly brackets <code>{}</code> in the query, not regular parentheses.</p> </li> <li> <p>Click Run Queries</p> <p></p> <p>Your graph is now displaying the memory usage over time for your own project. If you see a \u201cNo datapoints found\u201d message, select a longer timeframe using the dropdown menu in the top left of the graph.</p> <p>If you skipped ahead to this lab without completing the others, it\u2019s possible that your project has not had workload deployed in it for more than the maximum time frame. If this is the case, run a simple application in your project, and you will see the data start to populate.</p> </li> </ol> <p>As you might have noticed, working directly with Prometheus can be tedious and requires specific PromQL queries that aren\u2019t the easiest to work with. That\u2019s why people typically use Prometheus for its data source functionality, and then move to Grafana for the data visualization.</p>"},{"location":"monitoring/#openshift-monitoring-grafana","title":"OpenShift Monitoring (Grafana)","text":"<ol> <li> <p>From the OpenShift menu, navigate to Observability -&gt; Dashboards.</p> <p></p> <p>This takes you to an in-browser user interface for the Grafana monitoring solution. By default, there are various preconfigured dashboards for common use cases.</p> <p></p> </li> <li> <p>Click the \"Dashboard\" dropdown in the top-left of the page, and select <code>Kubernetes / Compute Resources / Cluster</code>.</p> <p></p> <p>You will see a dashboard populated with information related to the cluster\u2019s compute resources such as CPU and memory utilization. This dashboard displays CPU usage and CPU quota/memory requests by namespace.</p> </li> <li> <p>Scroll down the page and look through the various CPU/memory/networking metrics.</p> <p>Notice that each chart has an <code>Inspect</code> option. Clicking it will take you to the relevant Metrics page with the proper Prometheus query applied for more fine-grained detail.</p> </li> </ol> <p>For users who are familiar with the standard Grafana interface, that can also be accessed outside of the OpenShift console (rather than these pages integrated into the console).</p> <p>You can find the direct link to the Grafana webpages by looking at the routes (Adminstrator -&gt; Networking -&gt; Routes) in the <code>openshift-monitoring</code> project.</p>"},{"location":"monitoring/#openshift-alerting-alertmanager","title":"OpenShift Alerting (AlertManager)","text":"<p>Alerting with Prometheus is separated into two parts. Alerting rules in Prometheus send alerts to Alertmanager. Alertmanager then manages those alerts, including silencing, inhibition, aggregation and sending out notifications via methods such as email or chat platforms like Slack.</p> <p></p> <p>An example rules file with an alert would be:</p> <pre><code>groups:\n- name: example\n  rules:\n  - alert: HighRequestLatency\n    expr: job:request_latency_seconds:mean5m{job=\"myjob\"} &gt; 0.5\n    for: 10m\n    labels:\n      severity: page\n    annotations:\n      summary: High request latency\n</code></pre> <p>The optional <code>for</code> clause causes Prometheus to wait for a certain duration between first encountering a new expression output vector element and counting an alert as firing for this element. In this case, Prometheus will check that the alert continues to be active during each evaluation for 10 minutes before firing the alert. Elements that are active, but not firing yet, are in the pending state.</p> <p>The <code>labels</code> clause allows specifying a set of additional labels to be attached to the alert. Any existing conflicting labels will be overwritten.</p> <p>The <code>annotations</code> clause specifies a set of informational labels that can be used to store longer additional information such as alert descriptions or runbook links.</p> <ol> <li> <p>In the menu bar on the left side of the OpenShift console, click Observe and then Alerting.</p> <p>You will be taken to an Alertmanager interface within the OpenShift console.</p> <p></p> </li> <li> <p>Click the Alerting Rules tab to see the 100+ alerts that are not currently firing (hopefully!)</p> <p></p> <p>These alerts come pre-built with the monitoring stack, and they will start firing if triggered. This list includes alerts for critical operators going down, pods crash-looping, nodes being unreachable, and many more. Feel free to look through them.</p> </li> </ol> <p>These alerts are typically sent to an external tool where relevant administrators, developers, or site-reliability engineers will be notified that they are firing. </p> <p>You cannot see and forwarded alerts for this OpenShift, but as an example, see the image below of a Slack alert generated by OpenShift alerting.</p> <p></p>"},{"location":"openshift_data_foundation/","title":"Persistent Storage with OpenShift Data Foundation (ODF)","text":"<p>Red Hat OpenShift Data Foundation is software-defined storage that is optimized for container environments. It runs as an operator on OpenShift Container Platform to provide highly integrated and simplified persistent storage management for containers.</p> <p>Red Hat OpenShift Data Foundation is integrated into the latest Red Hat OpenShift Container Platform to address platform services, application portability, and persistence challenges. It provides a highly scalable backend for the next generation of cloud-native applications, built on a technology stack that includes Red Hat Ceph Storage, the Rook.io Operator, and NooBaa\u2019s Multicloud Object Gateway technology. OpenShift Data Foundation also supports Logical Volume Manager Storage for single node OpenShift clusters.</p> <p>Red Hat OpenShift Data Foundation provides a trusted, enterprise-grade application development environment that simplifies and enhances the user experience across the application lifecycle in a number of ways:</p> <ul> <li>Provides block storage for databases.</li> <li>Shared file storage for continuous integration, messaging, and data aggregation.</li> <li>Object storage for cloud-first development, archival, backup, and media storage.</li> <li>Scale applications and data exponentially.</li> <li>Attach and detach persistent data volumes at an accelerated rate.</li> <li>Stretch clusters across multiple data-centers or availability zones.</li> <li>Establish a comprehensive application container registry.</li> <li>Support the next generation of OpenShift workloads such as Data Analytics, Artificial Intelligence, Machine Learning, Deep Learning, and Internet of Things (IoT). </li> </ul> <p>In this lab, you will use ODF to store the National Park locations that are inside the MongoDB database pod. By making this database persistent, the data will continue to exist even if the MongoDB pod restarts, is moved to another node, or otherwise experiences an outage. </p>"},{"location":"openshift_data_foundation/#adding-persistence-to-the-mongo-database","title":"Adding Persistence to the Mongo Database","text":"<ol> <li> <p>In the Administrator perspective of the OpenShift console, navigate to Storage -&gt; PersistentVolumeClaims and click the Create PersistentVolumeClaim button.</p> </li> <li> <p>Fill out the form as follows:</p> <ul> <li>StorageClass: <code>ocs-storagecluster-cephfs</code></li> <li>PersistentVolumeClaim name: <code>pvc-userNN</code> (where <code>NN</code> is your user number)</li> <li>Access mode: <code>Shared access (RWX)</code></li> <li>Size: <code>500 MiB</code></li> <li>Volume Mode: <code>Filesystem</code></li> </ul> <p>and click Create.</p> <p>You now have a claim of 500MB out of the pool of filesystem storage provided by OpenShift Data Foundation.</p> <p>Before adding a persistent volume to the database pod, we must first remove the ephemeral volume that currently exists.</p> </li> <li> <p>From the OpenShift developer topology page, click the icon for the MongoDB application, then the link for the deployment.</p> <p></p> </li> <li> <p>Scroll down to the Volumes section, and remove the single volume named <code>mongodb-nationalparks-volume-1</code></p> <p></p> </li> <li> <p>Scroll back to top of the MongoDB deployment page, click the Actions dropdown, and select Add Storage.</p> <p></p> </li> <li> <p>Fill the form as follows:</p> <ul> <li>Storage type: <code>PersistentVolumeClaim</code></li> <li>PersistentVolumeClaim: <code>Use existing claim</code> -&gt; <code>pvc-userNN</code></li> <li>Device Path: <code>/var/lib/mongodb/data</code></li> </ul> <p>and click Save.</p> <p>Your MongoDB database is now persistent and the data will continue to exist after the pod restarts or is moved.</p> </li> <li> <p>Load the MongoDB database again. The data was deleted when you removed the existing volume.</p> <pre><code>oc exec $(oc get pods -l component=nationalparks | tail -n 1 | awk '{print $1;}') -- curl -s http://localhost:8080/ws/data/load\n</code></pre> </li> <li> <p>Delete your MongoDB pod so that it recycles. You can do so either via the OpenShift console, or by entering the following command in the <code>oc</code> CLI:</p> <pre><code>oc delete po -l deployment=mongodb-nationalparks\n</code></pre> </li> <li> <p>Once the pod is back up and running, refresh the webpage for your National Parks application and see that all of the icons and locations of parks have persisted through the pod restart.</p> </li> </ol>"},{"location":"openshift_data_foundation/#exploring-the-openshift-data-foundation-console","title":"Exploring the OpenShift Data Foundation Console","text":"<ol> <li> <p>In the OpenShift Administrator perspective, navigate to Storage -&gt; Data Foundation.</p> <p></p> <p>This is the main page for OpenShift Data Foundation where you can see the status of your storage system, its capacity, performance, and more.</p> </li> <li> <p>Navigate to the <code>Storage Systems</code> tab, and then click the <code>ocs-storagecluster-storagesystem</code> hyperlink.</p> <p>On the Block and File tab, you can see information about the capacity and performance of block and file storage. You should also see your <code>userNN-project</code> listed as a storage consumer under Used Capacity Breakdown. </p> <p></p> </li> </ol> <p>OpenShift Data Foundation is a powerful software-defined storage solution for storage administrators who need visibility into the consumption and performance of storage in an OpenShift environment. </p>"},{"location":"overview/","title":"OpenShift Overview","text":"<p>OpenShift Container Platform is an enterprise-grade container platform based on Kubernetes. OpenShift is designed to allow applications and the data centers that support them to expand from just a few machines and applications to thousands of machines that serve millions of clients.</p> <p>With its foundation in Kubernetes, OpenShift Container Platform incorporates the same technology that serves as the engine for massive telecommunications, streaming video, gaming, banking, and other applications. Its implementation in open Red Hat technologies lets you extend your containerized applications beyond a single cloud to on-premises and multi-cloud environments.</p> <p></p> <p>OpenShift Container Platform, commonly referred to as OpenShift or OCP, is a Kubernetes environment for managing the lifecycle of container-based applications and their dependencies on various computing platforms, such as bare metal, virtualized, on-premises, and in cloud. OpenShift deploys, configures and manages containers. OpenShift offers usability, stability, and customization of its components.</p> <p>OpenShift utilizes a number of computing resources, known as nodes. Each node runs a lightweight, secure operating system based on Red Hat Enterprise Linux (RHEL), known as Red Hat Enterprise Linux CoreOS (RHCOS).</p> <p>After a node is booted and configured, it obtains the CRI-O container runtime for managing and running the images of container workloads scheduled to it. The Kubernetes agent, or kubelet schedules container workloads on the node. The kubelet is responsible for registering the node with the cluster and receiving the details of container workloads.</p> <p>OpenShift configures and manages the networking, load balancing and routing of the cluster. OpenShift adds cluster services for monitoring the cluster health and performance, logging, and for managing upgrades.</p> <p>The container image registry and OperatorHub provide Red Hat certified products and community-built software for providing various application services within the cluster. These applications and services manage the applications deployed in the cluster, databases, frontends and user interfaces, application runtimes and business automation, and developer services for development and testing of container applications.</p> <p>You can manage applications within the cluster either manually by configuring deployments of containers running from pre-built images or through resources known as Operators. OpenShift can also build and deploy custom images from source code so you developers can focus on developing - without learning new skills such as containerizing their code.</p>"},{"location":"overview/#overview-of-the-openshift-web-console","title":"Overview of the OpenShift Web Console","text":"<p>The OpenShift Container Platform web console is a user interface accessible from a web browser.</p> <p>Developers can use the web console to visualize, browse, and manage the contents of projects.</p> <p></p> <p>Administrators can use the web console to manage and monitor applications running on the cluster, along with the cluster itself.</p> <p></p> <p>The web console can be customized to suit an organization's needs, and when you log into the web console, you will only see the cluster resources that are available to you as allowed by the OpenShift Role Based Access Control (RBAC).</p> <p>The web console runs as a group of pods on the control plane nodes in the <code>openshift-console</code> project, along with a service exposed as a route. Like any other OpenShift application the service is an internal load balancer that directs traffic to one of the OpenShift console pods. The route is what allows external access into the service, and it is the address you connect to when accessing the OpenShift web console.</p> <p>The OpenShift web console is a great example of how OpenShift itself is run and managed as pods inthe same way as workloads running on the cluster.</p>"},{"location":"overview/#connect-to-the-openshift-console","title":"Connect to the OpenShift Console","text":"<ol> <li> <p>Open a web browser such as Firefox or Google Chrome.</p> </li> <li> <p>In the browser, navigate to your OpenShift console. </p> <p>If you are going through this lab during a workshop provided by the IBM Z Washington Systems Center, you can find this address in the access page in this respository.</p> <p>The OpenShift console typically begins with <code>https://console-openshift-console-</code>. Reach out to your OpenShift administrator if you do not have this address.</p> <p>You will now see the OpenShift console login page.</p> <p></p> </li> <li> <p>Log in with your OpenShift credentials. These are also included on the access page in this repository.</p> </li> </ol>"},{"location":"mkdocs/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"mkdocs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"mkdocs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"}]}